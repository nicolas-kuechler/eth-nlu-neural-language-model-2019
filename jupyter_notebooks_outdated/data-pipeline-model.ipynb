{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190403\n"
     ]
    }
   ],
   "source": [
    "## import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRAIN = \"./data/sentences/sentences.train\"\n",
    "PATH_VALID = \"./data/sentences/sentences.eval\"\n",
    "PATH_VOCAB = \"./data/vocab.txt\"\n",
    "\n",
    "SPECIAL = {\n",
    "    \"bos\" : \"<bos>\",\n",
    "    \"eos\" : \"<eos>\",\n",
    "    \"pad\" : \"<pad>\"\n",
    "}\n",
    "\n",
    "SENTENCE_LENGTH = 30\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "\n",
    "WORD_EMBEDDINGS = None # TODO load pretrained word embeddings and check that it's working in combination with tf.function\n",
    "EMBEDDING_SIZE = 100\n",
    "LSTM_HIDDEN_STATE_SIZE= 512\n",
    "LSTM_OUTPUT_SIZE = 512\n",
    "\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 0.001\n",
    "GRADIENT_CLIPPING_NORM = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(input_file, output_file, top_k=None, special=None):  \n",
    "    '''\n",
    "    builds a vocubulary output_file of size top_k, taking the most frequent words \n",
    "    in the input_file and also adding the special symbols from the given dict\n",
    "    '''\n",
    "    with open(input_file) as f:\n",
    "        wordcount = Counter(f.read().split())\n",
    "        wordcount = wordcount.most_common(top_k-len(special)-1)\n",
    "        \n",
    "    with open(output_file, \"w\") as f:\n",
    "        for symbol in special.values():\n",
    "            f.write(f\"{symbol}\\n\")\n",
    "            \n",
    "        for word, _ in wordcount:\n",
    "            f.write(f\"{word}\\n\")\n",
    "            \n",
    "build_vocab(input_file=PATH_TRAIN, output_file=PATH_VOCAB, top_k=VOCAB_SIZE, special=SPECIAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_lookup(filename, unknown_value):\n",
    "    '''\n",
    "    builds lookup tables for the mapping: word (str) <--> wordId (int)\n",
    "    '''\n",
    "\n",
    "    word_to_index_table_initializer = tf.lookup.TextFileInitializer(filename, \n",
    "                                                      tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                                      tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER, \n",
    "                                                      delimiter=\" \")\n",
    "    \n",
    "    word_to_index_table = tf.lookup.StaticVocabularyTable(word_to_index_table_initializer, num_oov_buckets=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    index_to_word_table_initializer = tf.lookup.TextFileInitializer(filename,\n",
    "                                                          tf.int64, tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                                          tf.string, tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                                          delimiter=\" \")\n",
    "    index_to_word_table = tf.lookup.StaticHashTable(index_to_word_table_initializer, unknown_value)\n",
    "   \n",
    "    return word_to_index_table, index_to_word_table\n",
    "\n",
    "\n",
    "def build_dataset(filename, vocab):\n",
    "    '''\n",
    "    builds a dataset from the given file and vocabulary\n",
    "    '''\n",
    "    \n",
    "    # load dataset from text file\n",
    "    dataset = tf.data.TextLineDataset(filename)\n",
    "\n",
    "    # tokenize sentence\n",
    "    dataset = dataset.map(lambda sentence: tf.strings.split([sentence], sep=' ').values)\n",
    "\n",
    "    # add <bos> and <eos>\n",
    "    dataset = dataset.map(lambda sentence: tf.concat([[SPECIAL['bos']], sentence, [SPECIAL['eos']]], axis=0))\n",
    "\n",
    "    # filter out sentences longer than 30\n",
    "    dataset = dataset.filter(lambda sentence: tf.shape(sentence)[0] <= SENTENCE_LENGTH)\n",
    "\n",
    "    # pad all sentences to length 30\n",
    "    dataset = dataset.map(lambda sentence: tf.pad(sentence, [[0,SENTENCE_LENGTH - tf.shape(sentence)[0]]], mode='CONSTANT', constant_values=SPECIAL['pad']))\n",
    "    \n",
    "    # map words to id\n",
    "    dataset = dataset.map(lambda sentence: vocab.lookup(sentence))\n",
    "    \n",
    "    # map to sentence and labels\n",
    "    dataset = dataset.map(lambda sentence: (sentence, sentence[1:SENTENCE_LENGTH]))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,)\n",
      "(29,)\n",
      "tf.Tensor(\n",
      "[    0     9     6   145   119    29   142 19999    30   247     4    23\n",
      "   257   119   128    26    41   142     3    10     1     2     2     2\n",
      "     2     2     2     2     2     2], shape=(30,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[b'<bos>' b'``' b'i' b\"'ve\" b'never' b'had' b'any' b'<unk>' b'for'\n",
      " b'myself' b',' b'my' b'father' b'never' b'let' b'me' b'have' b'any' b'.'\n",
      " b\"''\" b'<eos>' b'<pad>' b'<pad>' b'<pad>' b'<pad>' b'<pad>' b'<pad>'\n",
      " b'<pad>' b'<pad>' b'<pad>'], shape=(30,), dtype=string)\n",
      "\n",
      "\n",
      "(64, 30)\n",
      "(64, 29)\n",
      "tf.Tensor(\n",
      "[    0     9     6   145   119    29   142 19999    30   247     4    23\n",
      "   257   119   128    26    41   142     3    10     1     2     2     2\n",
      "     2     2     2     2     2     2], shape=(30,), dtype=int64)\n",
      "(64, 29)\n",
      "tf.Tensor(\n",
      "[    9     6   145   119    29   142 19999    30   247     4    23   257\n",
      "   119   128    26    41   142     3    10     1     2     2     2     2\n",
      "     2     2     2     2     2], shape=(29,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "word_to_index_table, index_to_word_table = build_vocab_lookup(PATH_VOCAB, \"<unk>\")\n",
    "\n",
    "ds_train = build_dataset(PATH_TRAIN, vocab=word_to_index_table)\n",
    "\n",
    "for x in ds_train:\n",
    "    print(x[0].shape)\n",
    "    print(x[1].shape)\n",
    "    print(x[0])\n",
    "    print(index_to_word_table.lookup(x[0]))\n",
    "    break\n",
    "\n",
    "ds_train = ds_train.batch(BATCH_SIZE)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "for x in ds_train:\n",
    "    print(x[0].shape)\n",
    "    print(x[1].shape)\n",
    "    print(x[0][0,:])\n",
    "    print(x[1].shape)\n",
    "    print(x[1][0,:])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LanguageModel(Model):\n",
    "    def __init__(self, vocab_size, sentence_length, embedding_size, hidden_state_size, output_size, batch_size, word_embeddings=None):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        \n",
    "  \n",
    "        self.vocab_size = vocab_size\n",
    "        self.sentence_length = sentence_length\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_state_size = hidden_state_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if word_embeddings is not None:\n",
    "            # TODO this does not work yet with @tf.function\n",
    "            emb = tf.Variable(word_embeddings, name=None)\n",
    "            weights = [emb]\n",
    "            trainable = False\n",
    "        else:\n",
    "            weights = None\n",
    "            trainable = True\n",
    "        \n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim= vocab_size,\n",
    "            output_dim= embedding_size,\n",
    "            input_length= sentence_length,\n",
    "            weights = weights,\n",
    "            trainable = trainable,\n",
    "            \n",
    "            embeddings_initializer='uniform', # TODO [nku] ?\n",
    "            embeddings_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            embeddings_constraint=None,\n",
    "            mask_zero=False,\n",
    "        )\n",
    "        \n",
    "        self.lstm_cell = tf.keras.layers.LSTMCell(\n",
    "            # dimensionality of the output space\n",
    "            units=hidden_state_size,\n",
    "            kernel_initializer='glorot_uniform', # xavier initializer\n",
    "            name=\"lstm_cell\",\n",
    "            \n",
    "            activation='tanh',\n",
    "            recurrent_activation='hard_sigmoid',\n",
    "            use_bias=True,\n",
    "            recurrent_initializer='orthogonal',\n",
    "            bias_initializer='zeros',\n",
    "            unit_forget_bias=True,\n",
    "            kernel_regularizer=None,\n",
    "            recurrent_regularizer=None,\n",
    "            bias_regularizer=None,\n",
    "            kernel_constraint=None,\n",
    "            recurrent_constraint=None,\n",
    "            bias_constraint=None,\n",
    "            dropout=0.0,\n",
    "            recurrent_dropout=0.0,\n",
    "            implementation=1\n",
    "        )\n",
    "\n",
    "        # hidden state dimension to vocab size dimension\n",
    "        if output_size != hidden_state_size: \n",
    "    \n",
    "            self.projection_layer  = tf.keras.layers.Dense(\n",
    "                output_size,\n",
    "                input_shape=(None, output_size), \n",
    "                activation=None,\n",
    "                use_bias=False,\n",
    "                kernel_initializer='glorot_uniform', # xavier initializer\n",
    "                name=\"Wp\",\n",
    "\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )\n",
    "            \n",
    "            \n",
    "        self.softmax_layer  = tf.keras.layers.Dense(\n",
    "                vocab_size,\n",
    "                input_shape=(None, output_size), \n",
    "                activation=None,\n",
    "                use_bias=False,\n",
    "                kernel_initializer='glorot_uniform', # xavier initializer\n",
    "                name=\"W\",\n",
    "\n",
    "                bias_initializer='zeros',\n",
    "                kernel_regularizer=None,\n",
    "                bias_regularizer=None,\n",
    "                activity_regularizer=None,\n",
    "                kernel_constraint=None,\n",
    "                bias_constraint=None\n",
    "            )\n",
    "\n",
    "    def call(self, sentence_id_batch):\n",
    "        \n",
    "        print(f\"Language Model Call: Input={sentence_id_batch.shape}\")\n",
    "        \n",
    "        # TODO: check if this is really how a static version of the forward pass is done?\n",
    "        \n",
    "        # initialize lstm state\n",
    "        init_state = tf.zeros([self.batch_size, self.hidden_state_size])\n",
    "        state = (init_state, init_state)\n",
    "        \n",
    "        logits  = []\n",
    "        \n",
    "        # embedding layer -> gets a sentence as input and performs the embedding\n",
    "        sentence_embedding_batch = self.embedding(sentence_id_batch)\n",
    "        print(f'sentence embedding batch shape: {sentence_embedding_batch.shape}')\n",
    "        \n",
    "        for pos in range(self.sentence_length-1):\n",
    "            \n",
    "            # extract word -> identity returns a tensor of the same dimension\n",
    "            # dimensions: [batch, sentence length, embedding size]\n",
    "            # selects a slice of the cube -> every embedding, every sentence in the batch, but always\n",
    "            # one certain position\n",
    "            word_embedding_batch =  tf.identity(sentence_embedding_batch[:, pos, :], name=f\"word_{pos}\")\n",
    "            \n",
    "            # output \\in [batch_size, hidden_state_size]\n",
    "            # state  \\in [batch_size, hidden_state_size]\n",
    "            # lstm cell has two outputs: one for the prediction of the next word (y_t) and\n",
    "            # one for the state\n",
    "            output, state = self.lstm_cell(word_embedding_batch, state)\n",
    "            \n",
    "            # project y_t down to output size |vocab|\n",
    "            if self.output_size != self.hidden_state_size:\n",
    "                output = self.projection_layer(output) # \\in [batch_size, output_size]\n",
    "            \n",
    "            # apply softmax weights to obtain logits\n",
    "            output = self.softmax_layer(output) # \\in [batch_size, vocab_size]\n",
    "            \n",
    "            logits.append(output)\n",
    "        \n",
    "        # \\in [batch_size, sentence_length-1, vocab_size]\n",
    "        logits = tf.stack(logits, axis=1)\n",
    "        \n",
    "        # \\in [batch_size, sentence_length-1, vocab_size]\n",
    "        preds = tf.nn.softmax(logits, name=None)\n",
    "        \n",
    "        # print(f\"logits shape = {logits.shape}\") \n",
    "        # print(f\"preds shape = {preds.shape}\") \n",
    "        \n",
    "        return logits, preds  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModelError(tf.losses.Loss):\n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_pred must be logits        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "        \n",
    "        # average over batch and sentence length\n",
    "        # y_pred \\in [64, 29, 20'000]\n",
    "        # y_true \\in [64, 29]\n",
    "        # sparse softmax takes these inputs with different dimensions\n",
    "        # loss \\in [64, 29] -> for every word in every sentence in every batch\n",
    "        # we compute the loss\n",
    "        # math.reduce_mean sums up the entire matrix and divides by #elements\n",
    "        loss = tf.math.reduce_mean(loss)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # comment tf.function out for debugging \n",
    "def train_step(sentence, labels):\n",
    "    with tf.GradientTape() as tape: \n",
    "        # within this context all ops are recorded =>\n",
    "        # can calc gradient of any tensor computed in this context with respect to any trainable var\n",
    "            \n",
    "        logits, preds = model(sentence)\n",
    "        # print(f\"logits = {logits.shape}  preds = {preds.shape}\") \n",
    "\n",
    "        loss = loss_object(y_true=labels, y_pred=logits)\n",
    "\n",
    "        # print(f\"loss  {loss}\")\n",
    "    \n",
    "    # apply gradient clipping \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_gradients, _global_norm = tf.clip_by_global_norm(gradients, clip_norm=GRADIENT_CLIPPING_NORM, use_norm=None, name=None)\n",
    "    optimizer.apply_gradients(zip(clipped_gradients, model.trainable_variables))\n",
    "        \n",
    "    # feed metrics\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, logits)\n",
    "\n",
    "@tf.function\n",
    "def valid_step(sentence, labels):\n",
    "    logits, preds = model(sentence)\n",
    "    \n",
    "    loss = loss_object(y_true=labels, y_pred=logits)\n",
    "    \n",
    "    valid_loss(loss)\n",
    "    valid_accuracy(labels, logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer('./logs')\n",
    "\n",
    "with summary_writer.as_default():\n",
    "    \n",
    "    word_to_index_table, index_to_word_table = build_vocab_lookup(PATH_VOCAB, \"<unk>\")\n",
    "    ds_train = build_dataset(PATH_TRAIN, vocab=word_to_index_table)\n",
    "    ds_train = ds_train.batch(BATCH_SIZE)\n",
    "    \n",
    "    ds_valid = build_dataset(PATH_VALID, vocab=word_to_index_table)\n",
    "    ds_valid = ds_valid.batch(BATCH_SIZE)\n",
    "    \n",
    "    model = LanguageModel(vocab_size = VOCAB_SIZE, \n",
    "                          sentence_length =  SENTENCE_LENGTH, \n",
    "                          embedding_size = EMBEDDING_SIZE, \n",
    "                          hidden_state_size = LSTM_HIDDEN_STATE_SIZE, \n",
    "                          output_size = LSTM_OUTPUT_SIZE,\n",
    "                          batch_size = BATCH_SIZE,\n",
    "                          word_embeddings = WORD_EMBEDDINGS)\n",
    "\n",
    "    \n",
    "    loss_object = LanguageModelError()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "    valid_loss = tf.keras.metrics.Mean(name='valid_loss')\n",
    "    valid_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
    "    \n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        # TODO: should the padded part be masked? (excluded from loss)\n",
    "        \n",
    "        for sentence, labels in ds_train:\n",
    "            # sentence \\in [batch_size, sentence_length]\n",
    "            # labels \\in [batch_size, sentence_length-1]\n",
    "            # print(f\"sentence = {sentence.shape}   labels = {labels.shape}\")\n",
    "            print(\"train_step\")\n",
    "            train_step(sentence, labels)\n",
    "            \n",
    "            # TODO figure out how to properly log metrics to tensorboard\n",
    "            tf.summary.scalar('train_loss', data=train_loss.result(), step=epoch)\n",
    "\n",
    "            \n",
    "        for sentence, labels in ds_valid:\n",
    "            print(\"valid_step\")\n",
    "            valid_step(sentence, labels)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

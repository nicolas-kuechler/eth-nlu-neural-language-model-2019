{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Stuff that is usefull for understanding:\n",
    "https://www.tensorflow.org/alpha/tutorials/quickstart/advanced\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-dev20190403\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "path_train = \"./data/sentences/sentences.train\"\n",
    "path_vocab = \"./data/vocab.txt\"\n",
    "\n",
    "special = {\n",
    "    \"bos\" : \"<bos>\",\n",
    "    \"eos\" : \"<eos>\",\n",
    "    \"pad\" : \"<pad>\"\n",
    "}\n",
    "\n",
    "sentence_length = 30\n",
    "batch_size = 64\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(input_file, output_file, top_k=None, special=None):  \n",
    "    '''\n",
    "    builds a vocubulary output_file of size top_k, taking the most frequent words \n",
    "    in the input_file and also adding the special symbols from the given dict\n",
    "    '''\n",
    "    with open(input_file) as f:\n",
    "        wordcount = Counter(f.read().split())\n",
    "        wordcount = wordcount.most_common(top_k-len(special)-1)\n",
    "        \n",
    "    with open(output_file, \"w\") as f:\n",
    "        for symbol in special.values():\n",
    "            f.write(f\"{symbol}\\n\")\n",
    "            \n",
    "        for word, _ in wordcount:\n",
    "            f.write(f\"{word}\\n\")\n",
    "    \n",
    "build_vocab(input_file=path_train, output_file=path_vocab, top_k=20000, special=special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_lookup(filename, unknown_value):\n",
    "    '''\n",
    "    builds lookup tables for the mapping: word (str) <--> wordId (int)\n",
    "    '''\n",
    "\n",
    "    table_initializer = tf.lookup.TextFileInitializer(filename=filename,\n",
    "                                                        key_dtype=tf.string,\n",
    "                                                        key_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                                        value_dtype=tf.int64,\n",
    "                                                        value_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                                        vocab_size=None,\n",
    "                                                        delimiter=\" \")\n",
    "\n",
    "    word_to_index_table = tf.lookup.StaticVocabularyTable(table_initializer, num_oov_buckets=1)\n",
    "    \n",
    "    #vocab_size = word_to_index_table.size()\n",
    "    \n",
    "\n",
    "\n",
    "    index_to_word_table = tf.lookup.StaticHashTable(tf.lookup.TextFileInitializer(filename=filename,\n",
    "                                                        value_dtype=tf.string,\n",
    "                                                        value_index=tf.lookup.TextFileIndex.WHOLE_LINE,\n",
    "                                                        key_dtype=tf.int64,\n",
    "                                                        key_index=tf.lookup.TextFileIndex.LINE_NUMBER,\n",
    "                                                        vocab_size=None,\n",
    "                                                        delimiter=\" \"), '<oov>')\n",
    "    return word_to_index_table, index_to_word_table\n",
    "\n",
    "\n",
    "def build_dataset(filename, vocab):\n",
    "    '''\n",
    "    builds a dataset from the given file and vocabulary\n",
    "    '''\n",
    "    \n",
    "    # load dataset from text file\n",
    "    dataset = tf.data.TextLineDataset(filename)\n",
    "\n",
    "    # tokenize sentence\n",
    "    dataset = dataset.map(lambda sentence: tf.strings.split([sentence], sep=' ').values)\n",
    "\n",
    "    # add <bos> and <eos>\n",
    "    dataset = dataset.map(lambda sentence: tf.concat([[special['bos']], sentence, [special['eos']]], axis=0))\n",
    "\n",
    "    # filter out sentences longer than 30\n",
    "    dataset = dataset.filter(lambda sentence: tf.shape(sentence)[0] <= sentence_length)\n",
    "\n",
    "    # pad all sentences to length 30\n",
    "    dataset = dataset.map(lambda sentence: tf.pad(sentence, [[0,sentence_length - tf.shape(sentence)[0]]], mode='CONSTANT', constant_values=special['pad']))\n",
    "    \n",
    "    # map words to id\n",
    "    dataset = dataset.map(lambda sentence: vocab.lookup(sentence))\n",
    "    \n",
    "    #make x,y for dataset, so one has input and label\n",
    "    dataset = dataset.map(lambda sentence: (sentence[:-1],sentence[1:]))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 29)\n",
      "(64, 29)\n",
      "tf.Tensor(\n",
      "[    0     9     6   145   119    29   142 19999    30   247     4    23\n",
      "   257   119   128    26    41   142     3    10     1     2     2     2\n",
      "     2     2     2     2     2], shape=(29,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[    9     6   145   119    29   142 19999    30   247     4    23   257\n",
      "   119   128    26    41   142     3    10     1     2     2     2     2\n",
      "     2     2     2     2     2], shape=(29,), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(index_to_word_table.lookup(tf.convert_to_tensor([x for x in range(5)],dtype=tf.int64)))\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index_table,index_to_word_table = build_vocab_lookup(path_vocab, \"<unk>\")\n",
    "\n",
    "ds_train = build_dataset(path_train, vocab=word_to_index_table)\n",
    "\n",
    "ds_train = ds_train.batch(batch_size)\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer('./logs')\n",
    "\n",
    "with train_summary_writer.as_default():\n",
    "\n",
    "    for x,y in ds_train:\n",
    "        print(x.shape)\n",
    "        print(y.shape)\n",
    "        print(x[0,:])\n",
    "        print(y[0,:])\n",
    "        break\n",
    "\n",
    "'''\n",
    "print(index_to_word_table.lookup(tf.convert_to_tensor([x for x in range(5)],dtype=tf.int64)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self,embedding_size,state_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        \n",
    "    def call(self,x, state):\n",
    "        \n",
    "        return x,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "##TODO##TODO\n",
    "class Model(Model):\n",
    "    def __init__(self,vocab_size,embedding_size, state_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_size,\n",
    "            embeddings_initializer='uniform',\n",
    "            embeddings_regularizer=None,\n",
    "            activity_regularizer=None,\n",
    "            embeddings_constraint=None,\n",
    "            mask_zero=False,\n",
    "            input_length=None)\n",
    "        self.flatten = Flatten()\n",
    "        self.rnn = \n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "model = Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As asked by the task description\n",
    "loss_object = tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "#a single trainingstep\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x)\n",
    "        loss = loss_object(y, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for x, y in ds_train:\n",
    "        train_step(x, y)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

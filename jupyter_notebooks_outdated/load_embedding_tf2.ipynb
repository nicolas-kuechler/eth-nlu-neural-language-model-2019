{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Code that loads the pretrained embedding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(vocab, emb, path, dim_embedding, vocab_size):\n",
    "    '''\n",
    "      vocab          A dictionary mapping token strings to vocabulary IDs\n",
    "      emb            Embedding tensor of shape vocabulary_size x dim_embedding\n",
    "      path           Path to embedding file\n",
    "      dim_embedding  Dimensionality of the external embedding.\n",
    "    '''\n",
    "\n",
    "    print(\"Loading external embeddings from %s\" % path)\n",
    "\n",
    "    model = models.KeyedVectors.load_word2vec_format(path, binary=False)  \n",
    "    external_embedding = np.zeros(shape=(vocab_size, dim_embedding))\n",
    "    matches = 0\n",
    "\n",
    "    for tok, idx in vocab.items():\n",
    "        if tok in model.vocab:\n",
    "            external_embedding[idx] = model[tok]\n",
    "            matches += 1\n",
    "        else:\n",
    "            print(\"%s not in embedding file\" % tok)\n",
    "            external_embedding[idx] = np.random.uniform(low=-0.25, high=0.25, size=dim_embedding)\n",
    "        \n",
    "    print(\"%d words out of %d could be loaded\" % (matches, vocab_size))\n",
    "    \n",
    "    emb = external_embedding\n",
    "    return emb\n",
    "    \n",
    "    #placeholder is not needed anymore\n",
    "    #retrained_embeddings = tf.placeholder(tf.float32, [None, None]) \n",
    "    \n",
    "    #Update 'ref' by assigning 'value' to it.\n",
    "    #Update the value of `emb` to pretrained_embeddings -> which was only a placeholder.\n",
    "    #-> this placeholder was fed by the external embedding\n",
    "    #assign_op = emb.assign(pretrained_embeddings)\n",
    "    \n",
    "    #session.run(assign_op, {pretrained_embeddings: external_embedding}) # here, embeddings are actually set\n",
    "    \n",
    "    # the entire block is not needed anymore -> the only thing that it does is assigning the external \n",
    "    # embedding to the tensor 'emb'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
